#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Dec  7 17:41:38 2017

@author: sisyphus
"""

import os  
import shutil  
import numpy as np
from skimage import io,transform
import tensorflow as tf
from PIL import Image
import time
import matplotlib.pyplot as plt
import matplotlib.image as mimage

w=28
h=28   #图片大小
input_count=10   #分类数

data_dir='/Users/sisyphus/.spyder-py3/Ciscotest/train_dataset'   #原始数据文件夹
train_dir='/Users/sisyphus/.spyder-py3/Ciscotest/train_dataset_train'    #选取的训练数据
test_dir='/Users/sisyphus/.spyder-py3/Ciscotest/train_dataset_test'     #选取的测试数据
MODEL_SAVE_PATH='/Users/sisyphus/.spyder-py3/Ciscotest/model2'   #模型保存路径
MODEL_NAME='model.ckpt'    #模型保存文件名

directories = [d for d in os.listdir(data_dir) 
                   if os.path.isdir(os.path.join(data_dir, d))]

imgs_train=[]
imgs_test=[]
labels_train=[]
labels_test=[]
train_num=100   #训练集大小，这里为节约时间取了100
test_num=15     #测试集大小，同样为节约时间选取了15
k=0

for d in directories:
    #print(d)
    path = os.path.join(data_dir, d)
    train_path=os.path.join(train_dir, d)
    test_path=os.path.join(test_dir, d)
    for root, dirs, files in os.walk(path):
        if len(dirs) == 0:
            for i in range(0,train_num):
                #print(i)
                file_path = root+'/'+files[i]  
                img = Image.open(file_path)
                img=img.resize((w,h))
                img.save(os.path.join(train_path,os.path.basename(file_path)))
                img = np.array(img)
                new_img=img.reshape(img.shape+(1,))
                imgs_train.append(new_img)#训练数据集
                labels_train.append(k)
            
            for i in range(0,test_num):
                #print(i)
                file_path = root+'/'+files[i]  
                img = Image.open(file_path)
                img=img.resize((w,h))
                img.save(os.path.join(test_path,os.path.basename(file_path)))
                img = np.array(img)
                new_img=img.reshape(img.shape+(1,))
                imgs_test.append(new_img)#测试集
                labels_test.append(k)
    k+=1
                
    
x_train=imgs_train
y_train=labels_train
x_test=imgs_test
y_test=labels_test


#打乱顺序
num_example=len(x_train)
arr=np.arange(num_example)
np.random.shuffle(arr)
data=np.array(x_train)[arr]
label=np.array(y_train)[arr]

#将训练数据集分为训练集和验证集
ratio=0.8
s=np.int(num_example*ratio)
x_train=data[:s]
y_train=label[:s]
x_val=data[s:]
y_val=label[s:]


#-----------------构建网络----------------------
#占位符
x=tf.placeholder(tf.float32,shape=[None,w,h,1],name='x')
y_=tf.placeholder(tf.int32,shape=[None,],name='y_')
y_=tf.cast(y_,tf.int32)

#第一个卷积层
conv1=tf.layers.conv2d(
      inputs=x,
      filters=32,#28*28*32
      kernel_size=[5, 5],
      padding="same",
      activation=tf.nn.relu,
      kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))
pool1=tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)#14*14*32

#第二个卷积层
conv2=tf.layers.conv2d(
      inputs=pool1,
      filters=64,#14*14*64
      kernel_size=[5, 5],
      padding="same",
      activation=tf.nn.relu,
      kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))
pool2=tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)#7*7*64

#第三个卷积层
conv3=tf.layers.conv2d(
      inputs=pool2,
      filters=128,#7*7*128
      kernel_size=[3, 3],
      padding="same",
      activation=tf.nn.relu,
      kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))
pool3=tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], strides=2)#3*3*128


re1 = tf.reshape(pool3, [-1, 3 * 3 * 128])

#全连接层
dense1 = tf.layers.dense(inputs=re1, 
                      units=1024, 
                      activation=tf.nn.relu,
                      kernel_initializer=tf.truncated_normal_initializer(stddev=0.01),
                      kernel_regularizer=tf.contrib.layers.l2_regularizer(0.003))
dense2= tf.layers.dense(inputs=dense1, 
                      units=512, 
                      activation=tf.nn.relu,
                      kernel_initializer=tf.truncated_normal_initializer(stddev=0.01),
                      kernel_regularizer=tf.contrib.layers.l2_regularizer(0.003))
logits= tf.layers.dense(inputs=dense2, 
                        units=10, 
                        activation=None,
                        kernel_initializer=tf.truncated_normal_initializer(stddev=0.01),
                        kernel_regularizer=tf.contrib.layers.l2_regularizer(0.003))


loss=tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_,logits=logits)
train_op=tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)
correct_prediction = tf.equal(tf.cast(tf.argmax(logits,1),tf.int32), y_)    
acc= tf.reduce_mean(tf.cast(correct_prediction, tf.float32))


#使用生成器函数按批次取数据
def minibatches(inputs=None, targets=None, batch_size=None, shuffle=False):
    assert len(inputs) == len(targets)
    inputs=np.array(inputs)
    targets=np.array(targets)
    if shuffle:
        indices = np.arange(len(inputs))
        np.random.shuffle(indices)
    for start_idx in range(0, len(inputs) - batch_size + 1, batch_size):
        if shuffle:
            excerpt = indices[start_idx:start_idx + batch_size]
        else:
            excerpt = slice(start_idx, start_idx + batch_size)
        yield inputs[excerpt], targets[excerpt]



n_epoch=30   #训练轮数，这里为节约时间改小了
batch_size=20     #批大小
sess=tf.InteractiveSession()  
sess.run(tf.global_variables_initializer())
train_acct=[]
val_acct=[]
test_acct=[]
n_batcht=[]
saver=tf.train.Saver()
for epoch in range(n_epoch):
    start_time = time.time()
    print(epoch)
    #training
    train_loss, train_acc, n_batch = 0, 0, 0
    for x_train_a, y_train_a in minibatches(x_train, y_train, batch_size, shuffle=True):
        _,err,ac=sess.run([train_op,loss,acc], feed_dict={x: x_train_a, y_: y_train_a})
        train_loss += err; train_acc += ac; n_batch += 1
    print("   train acc: %f" % (train_acc/ n_batch))
    train_acct.append(train_acc/n_batch)
    n_batcht.append(epoch)
 
    
    #val
    val_loss, val_acc, n_batch = 0, 0, 0
    for x_val_a, y_val_a in minibatches(x_val, y_val, batch_size, shuffle=False):
        err, ac = sess.run([loss,acc], feed_dict={x: x_val_a, y_: y_val_a})
        val_loss += err; val_acc += ac; n_batch += 1
    print("   val acc: %f" % (val_acc/ n_batch))
    val_acct.append(val_acc/n_batch)
    
    #test
    test_loss, test_acc, n_batch = 0, 0, 0
    for x_test_a, y_test_a in minibatches(x_test, y_test, batch_size, shuffle=False):
        err, ac = sess.run([loss,acc], feed_dict={x: x_test_a, y_: y_test_a})
        test_loss += err; test_acc += ac; n_batch += 1
    print("   test acc: %f" % (test_acc/ n_batch))
    test_acct.append(test_acc/n_batch)
    
    if epoch%10==0:
        saver.save(sess,os.path.join(MODEL_SAVE_PATH,MODEL_NAME),global_step=epoch+1)
        print('model is saved')

sess.close()

plt.plot(n_batcht,train_acct,label="train_acc")
plt.plot(n_batcht,val_acct,label="val_acc")
plt.plot(n_batcht,test_acct,label="test_acc")

